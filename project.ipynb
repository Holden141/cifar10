{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels),(_,_) = tf.keras.datasets.cifar10.load_data() # (-,-)) we don't need test data\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 32, 32, 3).astype('float32') \n",
    "train_images = (train_images - 127.5)/ 127.5 # Normalize the images to [-1, 1]\n",
    "\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(4*4*256, use_bias=False, input_shape=(100,))) # 4*4*256 = 4096\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((4, 4, 256))) # 4*4*256 = 4096\n",
    "    assert model.output_shape == (None, 4, 4, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False)) # 8*8*128 = 8192\n",
    "    assert model.output_shape == (None, 8, 8, 128)\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)) # 16*16*64 = 16384\n",
    "    assert model.output_shape == (None, 16, 16, 64)\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')) # 32*32*3 = 3072\n",
    "    assert model.output_shape == (None, 32, 32, 3)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#use generator (untrained)) to create an image\n",
    "generator = make_generator_model()\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m#use the untrained discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m discriminator \u001b[39m=\u001b[39m make_discriminator_model()\n\u001b[0;32m---> 18\u001b[0m decision \u001b[39m=\u001b[39m discriminator(generated_image)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generated_image' is not defined"
     ]
    }
   ],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[32, 32, 3])) # 16*16*64 = 16384\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')) # 8*8*128 = 8192 # 5,5 = kernel size\n",
    "    model.add(tf.keras.layers.LeakyReLU()) \n",
    "    model.add(tf.keras.layers.Dropout(0.3)) \n",
    "\n",
    "    model.add(layers.Flatten()) # 8*8*128 = 8192 \n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "#use the untrained discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images.\n",
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m checkpoint_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./training_checkpoints\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     16\u001b[0m checkpoint_prefix \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(checkpoint_dir, \u001b[39m\"\u001b[39m\u001b[39mckpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m checkpoint \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mCheckpoint(generator_optimizer\u001b[39m=\u001b[39mgenerator_optimizer,discriminator_optimizer\u001b[39m=\u001b[39mdiscriminator_optimizer,generator\u001b[39m=\u001b[39mgenerator,discriminator\u001b[39m=\u001b[39mdiscriminator)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,discriminator_optimizer=discriminator_optimizer,generator=generator,discriminator=discriminator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop will begin with the *generator* receiving a random seed as input. That seed is used to produce an image.\n",
    "The *discriminator* is then used to classify *real images*(from the training set) and *fakes* (from the generator).  \n",
    "The _loss_ is calculated for each and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50 # 50 epochs to train the model\n",
    "noise_dim = 100 # 100 random numbers to generate an image\n",
    "num_examples_to_generate = 16 #\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim]) # seed for the generator that we will use to visualize progress in the animated GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim]) # generate noise\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: # use gradient tape to record gradients\n",
    "        generated_images = generator(noise, training=True) # generate fake images\n",
    "\n",
    "        real_output = discriminator(images, training=True) # train discriminator on real images\n",
    "        fake_output = discriminator(generated_images, training=True) # train discriminator on fake images\n",
    "\n",
    "        gen_loss = generator_loss(fake_output) # calculate generator loss\n",
    "        disc_loss = discriminator_loss(real_output, fake_output) # calculate discriminator loss\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables) \n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        #produce images for the GIF\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,epoch + 1,seed)\n",
    "\n",
    "        #save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "        #generate after the final epoch\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,epochs,seed)\n",
    "\n",
    "\n",
    "def generate_and_save_images(model,epoch,test_input):\n",
    "    #training is set to False so all layers run in inference mode (batchnorm)\n",
    "    predictions = model(test_input, training=False)\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        plt.imshow(predictions[i, :, :, :] * 127.5 + 127.5)\n",
    "        plt.axis('off')\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "\n",
    "anim_file = 'dcgan.gif' #name of the file\n",
    "with imageio.get_writer(anim_file, mode='I') as writer: #get_writer() returns a writer object that will call append_data() to add a new frame to the GIF file.\n",
    "    filenames = glob.glob('image*.png') #glob.glob() returns a list of files that match the given pattern\n",
    "    filenames = sorted(filenames) \n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename) #imageio.imread() reads an image from the file\n",
    "        writer.append_data(image) #append_data() adds a new frame to the GIF file\n",
    "    image = imageio.imread(filename)\n",
    "    writer = append_data(image) #append_data() adds a new frame to the GIF file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
